{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0bcdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install datasets\n",
    "!pip install tokenizers==0.9.4\n",
    "!pip install transformers==4.2.1\n",
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4adb829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import logging\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "model.to(device)\n",
    "\n",
    "# make sure GPT2 appends EOS in begin and end\n",
    "def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "    outputs = [self.bos_token_id] + token_ids_0 + [self.eos_token_id]\n",
    "    return outputs\n",
    "\n",
    "GPT2Tokenizer.build_inputs_with_special_tokens = build_inputs_with_special_tokens\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# load train and validation data\n",
    "text_df = pd.read_csv('rrs-mimiciii/all/train.findings.tok', sep=\"delimiter\", header=None, names=['text'])\n",
    "summary_df = pd.read_csv('rrs-mimiciii/all/train.impression.tok',sep='delimiter', header=None, names=['summary'])\n",
    "train_df = pd.concat([text_df,summary_df], axis=1, join='inner')\n",
    "\n",
    "text_df = pd.read_csv('rrs-mimiciii/all/validate.findings.tok', sep=\"delimiter\", header=None, names=['text'])\n",
    "summary_df = pd.read_csv('rrs-mimiciii/all/validate.impression.tok',sep='delimiter', header=None, names=['summary'])\n",
    "valid_df = pd.concat([text_df,summary_df], axis=1, join='inner')\n",
    "\n",
    "text_df = pd.read_csv('rrs-mimiciii/all/test.findings.tok', sep=\"delimiter\", header=None, names=['text'])\n",
    "summary_df = pd.read_csv('rrs-mimiciii/all/test.impression.tok',sep='delimiter', header=None, names=['summary'])\n",
    "test_df = pd.concat([text_df,summary_df], axis=1, join='inner')\n",
    "\n",
    "train_dataset = datasets.Dataset.from_dict({\"text\":train_df['text'].tolist(),\"summary\":train_df['summary'].tolist()})\n",
    "val_dataset = datasets.Dataset.from_dict({\"text\":valid_df['text'].tolist(),\"summary\":valid_df['summary'].tolist()})\n",
    "\n",
    "train_dataset = train_dataset.select(range(512))\n",
    "val_dataset = val_dataset.select(range(128))\n",
    "\n",
    "encoder_length = 512\n",
    "decoder_length = 256\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "# map data correctly\n",
    "def map_to_encoder_decoder_inputs(batch):    # Tokenizer will automatically set [BOS] <text> [EOS] \n",
    "    # use bert tokenizer here for encoder\n",
    "    inputs = bert_tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=encoder_length)\n",
    "    # force summarization <= 128\n",
    "    outputs = gpt2_tokenizer(batch[\"summary\"], padding=\"max_length\", truncation=True, max_length=decoder_length)\n",
    "\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "    batch[\"decoder_input_ids\"] = outputs.input_ids\n",
    "    batch[\"labels\"] = outputs.input_ids.copy()\n",
    "    batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
    "\n",
    "    # complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not\n",
    "    batch[\"labels\"] = [\n",
    "        [-100 if mask == 0 else token for mask, token in mask_and_tokens] for mask_and_tokens in [zip(masks, labels) for masks, labels in zip(batch[\"decoder_attention_mask\"], batch[\"labels\"])]\n",
    "    ]\n",
    "\n",
    "    assert all([len(x) == encoder_length for x in inputs.input_ids])\n",
    "    assert all([len(x) == decoder_length for x in outputs.input_ids])\n",
    "\n",
    "    return batch\n",
    "\n",
    "# make train dataset ready\n",
    "train_dataset = train_dataset.map(\n",
    "    map_to_encoder_decoder_inputs, batched=True, batch_size=batch_size, remove_columns=[\"text\", \"summary\"],\n",
    ")\n",
    "train_dataset.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")\n",
    "\n",
    "# same for validation dataset\n",
    "val_dataset = val_dataset.map(\n",
    "    map_to_encoder_decoder_inputs, batched=True, batch_size=batch_size, remove_columns=[\"text\", \"summary\"],\n",
    ")\n",
    "val_dataset.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")\n",
    "\n",
    "\n",
    "# load rouge for validation\n",
    "rouge = datasets.load_metric(\"rouge\", experiment_id=1)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = gpt2_tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = gpt2_tokenizer.eos_token_id\n",
    "    label_str = gpt2_tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    res = {key: value.mid.fmeasure * 100 for key, value in rouge_output.items()}\n",
    "\n",
    "    return {k: round(v, 4) for k, v in res.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "124db20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9760fc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/scratch/vgvinodv_root/vgvinodv0/varu/jupyter/gpt2/\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=4,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    #push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
