{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4926ef0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting datasets==2.1.0\n",
      "  Downloading datasets-2.1.0-py3-none-any.whl (325 kB)\n",
      "\u001b[K     |████████████████████████████████| 325 kB 17.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from datasets==2.1.0) (21.0)\n",
      "Requirement already satisfied: responses<0.19 in /home/varu/.local/lib/python3.9/site-packages (from datasets==2.1.0) (0.18.0)\n",
      "Requirement already satisfied: dill in /home/varu/.local/lib/python3.9/site-packages (from datasets==2.1.0) (0.3.6)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /home/varu/.local/lib/python3.9/site-packages (from datasets==2.1.0) (11.0.0)\n",
      "Requirement already satisfied: aiohttp in /home/varu/.local/lib/python3.9/site-packages (from datasets==2.1.0) (3.8.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from datasets==2.1.0) (2.26.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from datasets==2.1.0) (4.62.3)\n",
      "Requirement already satisfied: pandas in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from datasets==2.1.0) (1.3.4)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/varu/.local/lib/python3.9/site-packages (from datasets==2.1.0) (2023.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from datasets==2.1.0) (1.20.3)\n",
      "Requirement already satisfied: multiprocess in /home/varu/.local/lib/python3.9/site-packages (from datasets==2.1.0) (0.70.14)\n",
      "Requirement already satisfied: xxhash in /home/varu/.local/lib/python3.9/site-packages (from datasets==2.1.0) (3.2.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /home/varu/.local/lib/python3.9/site-packages (from datasets==2.1.0) (0.13.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/varu/.local/lib/python3.9/site-packages (from aiohttp->datasets==2.1.0) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/varu/.local/lib/python3.9/site-packages (from aiohttp->datasets==2.1.0) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from aiohttp->datasets==2.1.0) (21.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/varu/.local/lib/python3.9/site-packages (from aiohttp->datasets==2.1.0) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/varu/.local/lib/python3.9/site-packages (from aiohttp->datasets==2.1.0) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/varu/.local/lib/python3.9/site-packages (from aiohttp->datasets==2.1.0) (1.8.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from aiohttp->datasets==2.1.0) (2.0.4)\n",
      "Requirement already satisfied: filelock in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==2.1.0) (3.3.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==2.1.0) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/varu/.local/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==2.1.0) (4.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from packaging->datasets==2.1.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.1.0) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.1.0) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.1.0) (2021.10.8)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from pandas->datasets==2.1.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from pandas->datasets==2.1.0) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets==2.1.0) (1.16.0)\n",
      "Installing collected packages: datasets\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.11.0\n",
      "    Uninstalling datasets-2.11.0:\n",
      "      Successfully uninstalled datasets-2.11.0\n",
      "Successfully installed datasets-2.1.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers==4.18.0\n",
      "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 17.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from transformers==4.18.0) (2021.8.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from transformers==4.18.0) (4.62.3)\n",
      "Requirement already satisfied: requests in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from transformers==4.18.0) (2.26.0)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
      "\u001b[K     |████████████████████████████████| 880 kB 105.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from transformers==4.18.0) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from transformers==4.18.0) (1.20.3)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 96.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from transformers==4.18.0) (21.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/varu/.local/lib/python3.9/site-packages (from transformers==4.18.0) (0.13.4)\n",
      "Requirement already satisfied: filelock in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from transformers==4.18.0) (3.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/varu/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0) (4.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from packaging>=20.0->transformers==4.18.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests->transformers==4.18.0) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests->transformers==4.18.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests->transformers==4.18.0) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests->transformers==4.18.0) (2.0.4)\n",
      "Requirement already satisfied: six in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from sacremoses->transformers==4.18.0) (1.16.0)\n",
      "Requirement already satisfied: click in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from sacremoses->transformers==4.18.0) (8.0.3)\n",
      "Requirement already satisfied: joblib in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from sacremoses->transformers==4.18.0) (1.1.0)\n",
      "Building wheels for collected packages: sacremoses\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=7f0121f426fd8118a1d4ee055528bcef53e1ec86a9534df673b7723319f24bc0\n",
      "  Stored in directory: /home/varu/.cache/pip/wheels/12/1c/3d/46cf06718d63a32ff798a89594b61e7f345ab6b36d909ce033\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: tokenizers, sacremoses, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.3\n",
      "    Uninstalling tokenizers-0.13.3:\n",
      "      Successfully uninstalled tokenizers-0.13.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.27.4\n",
      "    Uninstalling transformers-4.27.4:\n",
      "      Successfully uninstalled transformers-4.27.4\n",
      "Successfully installed sacremoses-0.0.53 tokenizers-0.12.1 transformers-4.18.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: rouge_score in /home/varu/.local/lib/python3.9/site-packages (0.1.2)\n",
      "Requirement already satisfied: six>=1.14.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: absl-py in /home/varu/.local/lib/python3.9/site-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from rouge_score) (3.6.5)\n",
      "Requirement already satisfied: numpy in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from rouge_score) (1.20.3)\n",
      "Requirement already satisfied: click in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from nltk->rouge_score) (8.0.3)\n",
      "Requirement already satisfied: joblib in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from nltk->rouge_score) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from nltk->rouge_score) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from nltk->rouge_score) (4.62.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets==2.1.0\n",
    "\n",
    "!pip install transformers==4.18.0\n",
    "#!pip install transformers==4.2.1\n",
    "\n",
    "!pip install rouge_score\n",
    "\n",
    "import datasets\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca4ce113",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd8b0003cdd44f54abc841dce1d4d204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4989ebd80f2d43c782c31a3b60907e7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2996bb64947b47c2b17c918643bb4d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7929755dc6a4385853367b28cb46660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33173327872c4bfb93e1c45239cd6afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.51k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99993926feaf49e4aed439c23f314df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset cnn_dailymail/3.0.0 (download: 558.32 MiB, generated: 1.28 GiB, post-processed: Unknown size, total: 1.82 GiB) to /home/varu/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "801b58e0d5c5474fa37a3a573502f6db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b20d3d0e6e04ab9ae6a58b09cdcebb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/159M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4359de1de40b48139501195f029cb05f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/376M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12dabf40c96b4f98b47e4b8997cd0abe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/572k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3acfb37577d64a0eb14cda3c8d3b338f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/12.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38378cccd1f04146b479b9c8f82f533e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/661k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81e0833ce0847619cc46e56f70538d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cnn_dailymail downloaded and prepared to /home/varu/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset cnn_dailymail (/home/varu/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.bos_token = tokenizer.cls_token\n",
    "tokenizer.eos_token = tokenizer.sep_token\n",
    "\n",
    "train_data = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train\")\n",
    "val_data = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"validation[:10%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062f33ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function process_data_to_model_inputs at 0x1518a9393e50> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda9d06c4e02411b909d73582658f1ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/71779 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size=4  # change to 16 for full training\n",
    "encoder_max_length=512\n",
    "decoder_max_length=128\n",
    "\n",
    "def process_data_to_model_inputs(batch):\n",
    "  # tokenize the inputs and labels\n",
    "  inputs = tokenizer(batch[\"article\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n",
    "  outputs = tokenizer(batch[\"highlights\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n",
    "\n",
    "  batch[\"input_ids\"] = inputs.input_ids\n",
    "  batch[\"attention_mask\"] = inputs.attention_mask\n",
    "  batch[\"decoder_input_ids\"] = outputs.input_ids\n",
    "  batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
    "  batch[\"labels\"] = outputs.input_ids.copy()\n",
    "\n",
    "  # because BERT automatically shifts the labels, the labels correspond exactly to `decoder_input_ids`. \n",
    "  # We have to make sure that the PAD token is ignored\n",
    "  batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n",
    "\n",
    "  return batch\n",
    "\n",
    "# only use 32 training examples for notebook - DELETE LINE FOR FULL TRAINING\n",
    "#train_data = train_data.select(range(32))\n",
    "\n",
    "train_data = train_data.map(\n",
    "    process_data_to_model_inputs, \n",
    "    batched=True, \n",
    "    batch_size=batch_size, \n",
    "    remove_columns=[\"article\", \"highlights\", \"id\"]\n",
    ")\n",
    "train_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")\n",
    "\n",
    "\n",
    "# only use 16 training examples for notebook - DELETE LINE FOR FULL TRAINING\n",
    "#val_data = val_data.select(range(16))\n",
    "\n",
    "val_data = val_data.map(\n",
    "    process_data_to_model_inputs, \n",
    "    batched=True, \n",
    "    batch_size=batch_size, \n",
    "    remove_columns=[\"article\", \"highlights\", \"id\"]\n",
    ")\n",
    "val_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc420d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EncoderDecoderModel\n",
    "\n",
    "bert2bert = EncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-uncased\", \"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8989aace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set special tokens\n",
    "bert2bert.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "bert2bert.config.eos_token_id = tokenizer.eos_token_id\n",
    "bert2bert.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# sensible parameters for beam search\n",
    "bert2bert.config.vocab_size = bert2bert.config.decoder.vocab_size\n",
    "bert2bert.config.max_length = 142\n",
    "bert2bert.config.min_length = 56\n",
    "bert2bert.config.no_repeat_ngram_size = 3\n",
    "bert2bert.config.early_stopping = True\n",
    "bert2bert.config.length_penalty = 2.0\n",
    "bert2bert.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5d49af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c77209b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load rouge for validation\n",
    "rouge = datasets.load_metric(\"rouge\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886b3627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set training arguments - these params are not really tuned, feel free to change\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=1000,  # set to 1000 for full training\n",
    "    save_steps=500,  # set to 500 for full training\n",
    "    eval_steps=8000,  # set to 8000 for full training\n",
    "    warmup_steps=2000,  # set to 2000 for full training\n",
    "    #max_steps=16, # delete for full training\n",
    "    overwrite_output_dir=True,\n",
    "    save_total_limit=3,\n",
    "    fp16=True, \n",
    ")\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=bert2bert,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b5c266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from transformers import BertTokenizer, EncoderDecoderModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = EncoderDecoderModel.from_pretrained(\"./checkpoint-16\")\n",
    "model.to(\"cuda\")\n",
    "\n",
    "test_data = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test\")\n",
    "\n",
    "# only use 16 training examples for notebook - DELETE LINE FOR FULL TRAINING\n",
    "test_data = test_data.select(range(16))\n",
    "\n",
    "batch_size = 16  # change to 64 for full evaluation\n",
    "\n",
    "# map data correctly\n",
    "def generate_summary(batch):\n",
    "    # Tokenizer will automatically set [BOS] <text> [EOS]\n",
    "    # cut off at BERT max length 512\n",
    "    inputs = tokenizer(batch[\"article\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(\"cuda\")\n",
    "    attention_mask = inputs.attention_mask.to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # all special tokens including will be removed\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    batch[\"pred\"] = output_str\n",
    "\n",
    "    return batch\n",
    "\n",
    "results = test_data.map(generate_summary, batched=True, batch_size=batch_size, remove_columns=[\"article\"])\n",
    "\n",
    "pred_str = results[\"pred\"]\n",
    "label_str = results[\"highlights\"]\n",
    "\n",
    "rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "print(rouge_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a05e89e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
