{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54e0accf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install datasets\n",
    "!pip install tokenizers==0.9.4\n",
    "!pip install transformers==4.2.1\n",
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921418f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return func(*args, **kwargs)\n",
      "WARNING:datasets.fingerprint:Parameter 'function'=<function map_to_encoder_decoder_inputs at 0x149a8cf72a60> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b179de7169544e35a25e9d5db4709941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7415 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b286c1ae7ca4c3d9dd0e965e44faf5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/927 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='19532' max='22245' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19532/22245 1:46:38 < 14:48, 3.05 it/s, Epoch 2.63/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.404200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import datasets\n",
    "import logging\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, GPT2Tokenizer, EncoderDecoderModel\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "#model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-cased\", \"gpt2\")\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = EncoderDecoderModel.from_pretrained(\"patrickvonplaten/bert2gpt2-cnn_dailymail-fp16\")\n",
    "model.to(device)\n",
    "\n",
    "# cache is currently not supported by EncoderDecoder framework\n",
    "model.decoder.config.use_cache = False\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "# CLS token will work as BOS token\n",
    "bert_tokenizer.bos_token = bert_tokenizer.cls_token\n",
    "\n",
    "# SEP token will work as EOS token\n",
    "bert_tokenizer.eos_token = bert_tokenizer.sep_token\n",
    "\n",
    "\n",
    "# make sure GPT2 appends EOS in begin and end\n",
    "def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "    outputs = [self.bos_token_id] + token_ids_0 + [self.eos_token_id]\n",
    "    return outputs\n",
    "\n",
    "\n",
    "GPT2Tokenizer.build_inputs_with_special_tokens = build_inputs_with_special_tokens\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# set pad_token_id to unk_token_id -> be careful here as unk_token_id == eos_token_id == bos_token_id\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.unk_token\n",
    "\n",
    "\n",
    "# set decoding params\n",
    "model.config.decoder_start_token_id = gpt2_tokenizer.bos_token_id\n",
    "model.config.eos_token_id = gpt2_tokenizer.eos_token_id\n",
    "model.config.max_length = 142\n",
    "model.config.min_length = 56\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.early_stopping = True\n",
    "model.length_penalty = 2.0\n",
    "model.num_beams = 4\n",
    "\n",
    "# load train and validation data\n",
    "text_df = pd.read_csv('rrs-mimiciii/all/train.findings.tok', sep=\"delimiter\", header=None, names=['text'])\n",
    "summary_df = pd.read_csv('rrs-mimiciii/all/train.impression.tok',sep='delimiter', header=None, names=['summary'])\n",
    "train_df = pd.concat([text_df,summary_df], axis=1, join='inner')\n",
    "\n",
    "text_df = pd.read_csv('rrs-mimiciii/all/validate.findings.tok', sep=\"delimiter\", header=None, names=['text'])\n",
    "summary_df = pd.read_csv('rrs-mimiciii/all/validate.impression.tok',sep='delimiter', header=None, names=['summary'])\n",
    "valid_df = pd.concat([text_df,summary_df], axis=1, join='inner')\n",
    "\n",
    "text_df = pd.read_csv('rrs-mimiciii/all/test.findings.tok', sep=\"delimiter\", header=None, names=['text'])\n",
    "summary_df = pd.read_csv('rrs-mimiciii/all/test.impression.tok',sep='delimiter', header=None, names=['summary'])\n",
    "test_df = pd.concat([text_df,summary_df], axis=1, join='inner')\n",
    "\n",
    "train_dataset = datasets.Dataset.from_dict({\"text\":train_df['text'].tolist(),\"summary\":train_df['summary'].tolist()})\n",
    "val_dataset = datasets.Dataset.from_dict({\"text\":valid_df['text'].tolist(),\"summary\":valid_df['summary'].tolist()})\n",
    "\n",
    "#train_dataset = train_dataset.select(range(512))\n",
    "#val_dataset = val_dataset.select(range(128))\n",
    "\n",
    "# load rouge for validation\n",
    "rouge = datasets.load_metric(\"rouge\", experiment_id=1)\n",
    "\n",
    "encoder_length = 512\n",
    "decoder_length = 256\n",
    "batch_size = 8\n",
    "\n",
    "\n",
    "# map data correctly\n",
    "def map_to_encoder_decoder_inputs(batch):    # Tokenizer will automatically set [BOS] <text> [EOS] \n",
    "    # use bert tokenizer here for encoder\n",
    "    inputs = bert_tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=encoder_length)\n",
    "    # force summarization <= 128\n",
    "    outputs = gpt2_tokenizer(batch[\"summary\"], padding=\"max_length\", truncation=True, max_length=decoder_length)\n",
    "\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "    batch[\"decoder_input_ids\"] = outputs.input_ids\n",
    "    batch[\"labels\"] = outputs.input_ids.copy()\n",
    "    batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
    "\n",
    "    # complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not\n",
    "    batch[\"labels\"] = [\n",
    "        [-100 if mask == 0 else token for mask, token in mask_and_tokens] for mask_and_tokens in [zip(masks, labels) for masks, labels in zip(batch[\"decoder_attention_mask\"], batch[\"labels\"])]\n",
    "    ]\n",
    "\n",
    "    assert all([len(x) == encoder_length for x in inputs.input_ids])\n",
    "    assert all([len(x) == decoder_length for x in outputs.input_ids])\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = gpt2_tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = gpt2_tokenizer.eos_token_id\n",
    "    label_str = gpt2_tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    res = {key: value.mid.fmeasure * 100 for key, value in rouge_output.items()}\n",
    "\n",
    "    return {k: round(v, 4) for k, v in res.items()}\n",
    "\n",
    "\n",
    "# make train dataset ready\n",
    "train_dataset = train_dataset.map(\n",
    "    map_to_encoder_decoder_inputs, batched=True, batch_size=batch_size, remove_columns=[\"text\", \"summary\"],\n",
    ")\n",
    "train_dataset.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")\n",
    "\n",
    "# same for validation dataset\n",
    "val_dataset = val_dataset.map(\n",
    "    map_to_encoder_decoder_inputs, batched=True, batch_size=batch_size, remove_columns=[\"text\", \"summary\"],\n",
    ")\n",
    "val_dataset.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")\n",
    "\n",
    "# set training arguments - these params are not really tuned, feel free to change\n",
    "\n",
    "\n",
    "model.config.pad_token_id = model.config.decoder.eos_token_id\n",
    "\n",
    "'''\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    logging_steps=1000,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    overwrite_output_dir=True,\n",
    "    warmup_steps=2000,\n",
    "    save_total_limit=10,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "'''\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"/scratch/vgvinodv_root/vgvinodv0/varu/jupyter/\",\n",
    "    #evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    #per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    do_train=True,\n",
    "    do_eval=False,\n",
    "    predict_with_generate=True,\n",
    "    overwrite_output_dir=True,\n",
    "    logging_steps=1000,\n",
    "    #eval_steps=100,\n",
    "    warmup_steps=2000,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=bert_tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset = train_dataset,\n",
    "    #eval_dataset=val_dataset,\n",
    "    )\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd40782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import datasets\n",
    "from transformers import BertTokenizer, GPT2Tokenizer, EncoderDecoderModel\n",
    "\n",
    "#model = EncoderDecoderModel.from_pretrained(\"./checkpoint-16\")\n",
    "model.to(device)\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "# CLS token will work as BOS token\n",
    "bert_tokenizer.bos_token = bert_tokenizer.cls_token\n",
    "\n",
    "# SEP token will work as EOS token\n",
    "bert_tokenizer.eos_token = bert_tokenizer.sep_token\n",
    "\n",
    "\n",
    "# make sure GPT2 appends EOS in begin and end\n",
    "def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "    outputs = [self.bos_token_id] + token_ids_0 + [self.eos_token_id]\n",
    "    return outputs\n",
    "\n",
    "\n",
    "GPT2Tokenizer.build_inputs_with_special_tokens = build_inputs_with_special_tokens\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# set pad_token_id to unk_token_id -> be careful here as unk_token_id == eos_token_id == bos_token_id\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.unk_token\n",
    "\n",
    "\n",
    "# set decoding params\n",
    "model.config.decoder_start_token_id = gpt2_tokenizer.bos_token_id\n",
    "model.config.eos_token_id = gpt2_tokenizer.eos_token_id\n",
    "model.config.max_length = 142\n",
    "model.config.min_length = 56\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.early_stopping = True\n",
    "model.length_penalty = 2.0\n",
    "model.num_beams = 4\n",
    "\n",
    "test_dataset = datasets.Dataset.from_dict({\"text\":test_df['text'].tolist(),\"summary\":test_df['summary'].tolist()})\n",
    "#test_dataset = test_dataset.select(range(128))\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "# map data correctly\n",
    "def generate_summary(batch):\n",
    "    # Tokenizer will automatically set [BOS] <text> [EOS]\n",
    "    # cut off at BERT max length 512\n",
    "    inputs = bert_tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # all special tokens including will be removed\n",
    "    output_str = gpt2_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    batch[\"pred\"] = output_str\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "results = test_dataset.map(generate_summary, batched=True, batch_size=batch_size, remove_columns=[\"text\"])\n",
    "\n",
    "# load rouge for validation\n",
    "rouge = datasets.load_metric(\"rouge\")\n",
    "\n",
    "pred_str = results[\"pred\"]\n",
    "label_str = results[\"summary\"]\n",
    "\n",
    "rouge_output = rouge.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "res = {key: value.mid.fmeasure * 100 for key, value in rouge_output.items()}\n",
    "print({k: round(v, 4) for k, v in res.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b10f69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=10\n",
    "print(test_dataset['text'][sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508e3d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_str[sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d3d503",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_str[sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b530f7c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
